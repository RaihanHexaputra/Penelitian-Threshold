{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674d68be-dee1-4910-a28e-f08a102316c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library untuk pengolahan data dan visualisasi\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# Library untuk evaluasi dan model machine learning\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import sklearn.ensemble as ek\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Library untuk Explainable AI (XAI)\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import shap\n",
    "\n",
    "# Library untuk Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM, Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103abc4c-1b2e-4ca0-82e5-0b66b34b6afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Dataset \n",
    "\n",
    "DM = pd.read_csv(\"C:\\\\Data Raihan\\\\Penelitian Threshold\\\\Dataset\\\\CIC-PDFMal2022\\\\PDFMalware2022.csv\") #DM--> Dataset Malware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b14026-c76d-4c99-8f2a-723225d2df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10026 entries, 0 to 10025\n",
      "Data columns (total 33 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Fine name         10026 non-null  object \n",
      " 1   pdfsize           10025 non-null  float64\n",
      " 2   metadata size     10025 non-null  float64\n",
      " 3   pages             10025 non-null  float64\n",
      " 4   xref Length       10025 non-null  float64\n",
      " 5   title characters  10025 non-null  float64\n",
      " 6   isEncrypted       10025 non-null  float64\n",
      " 7   embedded files    10025 non-null  float64\n",
      " 8   images            10025 non-null  object \n",
      " 9   text              10025 non-null  object \n",
      " 10  header            10025 non-null  object \n",
      " 11  obj               10023 non-null  object \n",
      " 12  endobj            10023 non-null  object \n",
      " 13  stream            10023 non-null  float64\n",
      " 14  endstream         10023 non-null  object \n",
      " 15  xref              10023 non-null  object \n",
      " 16  trailer           10023 non-null  float64\n",
      " 17  startxref         10023 non-null  object \n",
      " 18  pageno            10023 non-null  object \n",
      " 19  encrypt           10023 non-null  float64\n",
      " 20  ObjStm            10023 non-null  float64\n",
      " 21  JS                10023 non-null  object \n",
      " 22  Javascript        10023 non-null  object \n",
      " 23  AA                10023 non-null  object \n",
      " 24  OpenAction        10023 non-null  object \n",
      " 25  Acroform          10023 non-null  object \n",
      " 26  JBIG2Decode       10023 non-null  object \n",
      " 27  RichMedia         10023 non-null  object \n",
      " 28  launch            10023 non-null  object \n",
      " 29  EmbeddedFile      10023 non-null  object \n",
      " 30  XFA               10023 non-null  object \n",
      " 31  Colors            10023 non-null  float64\n",
      " 32  Class             10025 non-null  object \n",
      "dtypes: float64(12), object(21)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "DM.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcbc54b-758f-4b1f-999c-71e61b807d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "X = DM.drop(['Fine name','images','text','header','obj','endobj','endstream','xref','startxref','pageno','JS','Javascript','AA','OpenAction','Acroform','JBIG2Decode','RichMedia','launch','EmbeddedFile','XFA','Class'],axis=1).values    #Droping this because classification model will not accept object type elements (float and int only)\n",
    "# Target variable\n",
    "y = DM['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bd6d6c-c0ef-470b-9c61-cf4044411438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Nan\n",
    "X = pd.DataFrame(X).dropna()\n",
    "y = y[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba75d9b8-e9cf-428b-bee5-0f0eb118955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Fitting and choosing the important variables\n",
    "extratrees = ek.ExtraTreesClassifier().fit(X,y)\n",
    "model = SelectFromModel(extratrees, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "nbfeatures = X_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa8ad04-c8ca-43d2-95fc-f272b05e0a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "index = np.argsort(extratrees.feature_importances_)[::-1][:nbfeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b2406f-e52d-412d-8796-a035eea7d0bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature text (0.239521)\n",
      "2. feature pages (0.195216)\n",
      "3. feature header (0.170515)\n",
      "4. feature metadata size (0.105700)\n"
     ]
    }
   ],
   "source": [
    "#All the required features\n",
    "for f in range(nbfeatures):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, DM.columns[2+index[f]], extratrees.feature_importances_[index[f]]))\n",
    "    features.append(DM.columns[2+f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdd0877-c2ba-4c27-a531-0e69ba6823e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Raihan\\AppData\\Local\\Temp\\ipykernel_19764\\1069642451.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_data = DM.groupby('Class').apply(lambda x: x.sample(frac=1)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Memilih 100% data secara acak dari setiap fitur/column\n",
    "sampled_data = DM.groupby('Class').apply(lambda x: x.sample(frac=1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8e4126-45f9-4e6f-b68b-75ae1712cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pisahkan data menjadi training dan testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1faf41d4-73c4-482f-a888-8a13d9dbef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels ke bentuk numerik jika diperlukan\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357dee1f-84d3-44cb-8081-df34898c5ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3865, number of negative: 3151\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 679\n",
      "[LightGBM] [Info] Number of data points in the train set: 7016, number of used features: 4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.550884 -> initscore=0.204242\n",
      "[LightGBM] [Info] Start training from score 0.204242\n",
      "0:\tlearn: 0.6225038\ttotal: 281ms\tremaining: 27.8s\n",
      "1:\tlearn: 0.5648435\ttotal: 321ms\tremaining: 15.7s\n",
      "2:\tlearn: 0.5146324\ttotal: 330ms\tremaining: 10.7s\n",
      "3:\tlearn: 0.4720224\ttotal: 382ms\tremaining: 9.18s\n",
      "4:\tlearn: 0.4350048\ttotal: 440ms\tremaining: 8.37s\n",
      "5:\tlearn: 0.4027545\ttotal: 477ms\tremaining: 7.47s\n",
      "6:\tlearn: 0.3755493\ttotal: 480ms\tremaining: 6.37s\n",
      "7:\tlearn: 0.3501847\ttotal: 481ms\tremaining: 5.54s\n",
      "8:\tlearn: 0.3271019\ttotal: 534ms\tremaining: 5.4s\n",
      "9:\tlearn: 0.3073735\ttotal: 565ms\tremaining: 5.08s\n",
      "10:\tlearn: 0.2902005\ttotal: 580ms\tremaining: 4.69s\n",
      "11:\tlearn: 0.2746216\ttotal: 602ms\tremaining: 4.42s\n",
      "12:\tlearn: 0.2600925\ttotal: 610ms\tremaining: 4.08s\n",
      "13:\tlearn: 0.2475987\ttotal: 619ms\tremaining: 3.8s\n",
      "14:\tlearn: 0.2359112\ttotal: 622ms\tremaining: 3.52s\n",
      "15:\tlearn: 0.2253975\ttotal: 664ms\tremaining: 3.49s\n",
      "16:\tlearn: 0.2167833\ttotal: 668ms\tremaining: 3.26s\n",
      "17:\tlearn: 0.2092614\ttotal: 672ms\tremaining: 3.06s\n",
      "18:\tlearn: 0.2017174\ttotal: 705ms\tremaining: 3.01s\n",
      "19:\tlearn: 0.1944779\ttotal: 711ms\tremaining: 2.84s\n",
      "20:\tlearn: 0.1880450\ttotal: 773ms\tremaining: 2.91s\n",
      "21:\tlearn: 0.1815289\ttotal: 786ms\tremaining: 2.79s\n",
      "22:\tlearn: 0.1763367\ttotal: 809ms\tremaining: 2.71s\n",
      "23:\tlearn: 0.1709611\ttotal: 814ms\tremaining: 2.58s\n",
      "24:\tlearn: 0.1669190\ttotal: 875ms\tremaining: 2.62s\n",
      "25:\tlearn: 0.1625019\ttotal: 983ms\tremaining: 2.8s\n",
      "26:\tlearn: 0.1586400\ttotal: 1.05s\tremaining: 2.85s\n",
      "27:\tlearn: 0.1561253\ttotal: 1.09s\tremaining: 2.81s\n",
      "28:\tlearn: 0.1534304\ttotal: 1.1s\tremaining: 2.7s\n",
      "29:\tlearn: 0.1505491\ttotal: 1.18s\tremaining: 2.75s\n",
      "30:\tlearn: 0.1471110\ttotal: 1.18s\tremaining: 2.63s\n",
      "31:\tlearn: 0.1442390\ttotal: 1.19s\tremaining: 2.54s\n",
      "32:\tlearn: 0.1412542\ttotal: 1.22s\tremaining: 2.48s\n",
      "33:\tlearn: 0.1387046\ttotal: 1.24s\tremaining: 2.41s\n",
      "34:\tlearn: 0.1360860\ttotal: 1.24s\tremaining: 2.31s\n",
      "35:\tlearn: 0.1347027\ttotal: 1.25s\tremaining: 2.22s\n",
      "36:\tlearn: 0.1329165\ttotal: 1.3s\tremaining: 2.21s\n",
      "37:\tlearn: 0.1316439\ttotal: 1.34s\tremaining: 2.19s\n",
      "38:\tlearn: 0.1304193\ttotal: 1.35s\tremaining: 2.11s\n",
      "39:\tlearn: 0.1289349\ttotal: 1.36s\tremaining: 2.04s\n",
      "40:\tlearn: 0.1277465\ttotal: 1.38s\tremaining: 1.99s\n",
      "41:\tlearn: 0.1260828\ttotal: 1.39s\tremaining: 1.92s\n",
      "42:\tlearn: 0.1247887\ttotal: 1.39s\tremaining: 1.85s\n",
      "43:\tlearn: 0.1235842\ttotal: 1.44s\tremaining: 1.84s\n",
      "44:\tlearn: 0.1218344\ttotal: 1.5s\tremaining: 1.83s\n",
      "45:\tlearn: 0.1203481\ttotal: 1.54s\tremaining: 1.8s\n",
      "46:\tlearn: 0.1195323\ttotal: 1.54s\tremaining: 1.74s\n",
      "47:\tlearn: 0.1178549\ttotal: 1.59s\tremaining: 1.72s\n",
      "48:\tlearn: 0.1170335\ttotal: 1.64s\tremaining: 1.71s\n",
      "49:\tlearn: 0.1160720\ttotal: 1.65s\tremaining: 1.65s\n",
      "50:\tlearn: 0.1155176\ttotal: 1.66s\tremaining: 1.6s\n",
      "51:\tlearn: 0.1139071\ttotal: 1.68s\tremaining: 1.55s\n",
      "52:\tlearn: 0.1132429\ttotal: 1.69s\tremaining: 1.5s\n",
      "53:\tlearn: 0.1120933\ttotal: 1.69s\tremaining: 1.44s\n",
      "54:\tlearn: 0.1111356\ttotal: 1.7s\tremaining: 1.39s\n",
      "55:\tlearn: 0.1103830\ttotal: 1.74s\tremaining: 1.37s\n",
      "56:\tlearn: 0.1099219\ttotal: 1.75s\tremaining: 1.32s\n",
      "57:\tlearn: 0.1089765\ttotal: 1.75s\tremaining: 1.27s\n",
      "58:\tlearn: 0.1086635\ttotal: 1.76s\tremaining: 1.22s\n",
      "59:\tlearn: 0.1077198\ttotal: 1.8s\tremaining: 1.2s\n",
      "60:\tlearn: 0.1077156\ttotal: 1.8s\tremaining: 1.15s\n",
      "61:\tlearn: 0.1073729\ttotal: 1.83s\tremaining: 1.12s\n",
      "62:\tlearn: 0.1063665\ttotal: 1.85s\tremaining: 1.08s\n",
      "63:\tlearn: 0.1063622\ttotal: 1.86s\tremaining: 1.05s\n",
      "64:\tlearn: 0.1058240\ttotal: 1.87s\tremaining: 1.01s\n",
      "65:\tlearn: 0.1056750\ttotal: 1.92s\tremaining: 990ms\n",
      "66:\tlearn: 0.1049726\ttotal: 1.93s\tremaining: 952ms\n",
      "67:\tlearn: 0.1041842\ttotal: 1.94s\tremaining: 915ms\n",
      "68:\tlearn: 0.1033431\ttotal: 1.98s\tremaining: 891ms\n",
      "69:\tlearn: 0.1022336\ttotal: 2.08s\tremaining: 893ms\n",
      "70:\tlearn: 0.1017337\ttotal: 2.13s\tremaining: 869ms\n",
      "71:\tlearn: 0.1015070\ttotal: 2.14s\tremaining: 831ms\n",
      "72:\tlearn: 0.1009685\ttotal: 2.15s\tremaining: 795ms\n",
      "73:\tlearn: 0.1009643\ttotal: 2.15s\tremaining: 756ms\n",
      "74:\tlearn: 0.1009566\ttotal: 2.15s\tremaining: 718ms\n",
      "75:\tlearn: 0.1008286\ttotal: 2.16s\tremaining: 683ms\n",
      "76:\tlearn: 0.1006039\ttotal: 2.2s\tremaining: 656ms\n",
      "77:\tlearn: 0.1003365\ttotal: 2.2s\tremaining: 621ms\n",
      "78:\tlearn: 0.1003323\ttotal: 2.2s\tremaining: 586ms\n",
      "79:\tlearn: 0.0999905\ttotal: 2.21s\tremaining: 552ms\n",
      "80:\tlearn: 0.0999842\ttotal: 2.25s\tremaining: 527ms\n",
      "81:\tlearn: 0.0999799\ttotal: 2.25s\tremaining: 494ms\n",
      "82:\tlearn: 0.0995468\ttotal: 2.25s\tremaining: 462ms\n",
      "83:\tlearn: 0.0992222\ttotal: 2.27s\tremaining: 432ms\n",
      "84:\tlearn: 0.0987468\ttotal: 2.29s\tremaining: 404ms\n",
      "85:\tlearn: 0.0986756\ttotal: 2.3s\tremaining: 374ms\n",
      "86:\tlearn: 0.0981420\ttotal: 2.35s\tremaining: 350ms\n",
      "87:\tlearn: 0.0980842\ttotal: 2.35s\tremaining: 321ms\n",
      "88:\tlearn: 0.0974691\ttotal: 2.36s\tremaining: 291ms\n",
      "89:\tlearn: 0.0972219\ttotal: 2.39s\tremaining: 266ms\n",
      "90:\tlearn: 0.0966257\ttotal: 2.4s\tremaining: 238ms\n",
      "91:\tlearn: 0.0959407\ttotal: 2.41s\tremaining: 210ms\n",
      "92:\tlearn: 0.0958526\ttotal: 2.42s\tremaining: 182ms\n",
      "93:\tlearn: 0.0955513\ttotal: 2.43s\tremaining: 155ms\n",
      "94:\tlearn: 0.0954358\ttotal: 2.44s\tremaining: 128ms\n",
      "95:\tlearn: 0.0953970\ttotal: 2.45s\tremaining: 102ms\n",
      "96:\tlearn: 0.0951661\ttotal: 2.46s\tremaining: 76ms\n",
      "97:\tlearn: 0.0951625\ttotal: 2.46s\tremaining: 50.2ms\n",
      "98:\tlearn: 0.0949176\ttotal: 2.46s\tremaining: 24.9ms\n",
      "99:\tlearn: 0.0949141\ttotal: 2.46s\tremaining: 0us\n",
      "94/94 [==============================] - 1s 5ms/step\n",
      "94/94 [==============================] - 1s 4ms/step\n",
      "94/94 [==============================] - 2s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ExactExplainer explainer: 3008it [01:20, 37.20it/s]                                                                    \n",
      "ExactExplainer explainer: 3008it [00:25, 117.10it/s]                                                                   \n",
      "ExactExplainer explainer: 3008it [00:14, 70.77it/s]                                                                    \n",
      "ExactExplainer explainer: 3008it [00:11, 36.44it/s]                                                                    \n",
      "ExactExplainer explainer:  64%|█████████████████████████████████▎                  | 1929/3007 [04:57<02:27,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 809us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:99: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\"Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\")\n",
      "C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:155: UserWarning: You have provided over 5k background samples! For better performance consider using smaller random sample.\n",
      "  warnings.warn(\"You have provided over 5k background samples! For better performance consider using smaller random sample.\")\n",
      "C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using SHAP with CNN: operands could not be broadcast together with shapes (7016,4,1) (7016,4) \n",
      "94/94 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:99: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\"Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\")\n",
      "C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:155: UserWarning: You have provided over 5k background samples! For better performance consider using smaller random sample.\n",
      "  warnings.warn(\"You have provided over 5k background samples! For better performance consider using smaller random sample.\")\n",
      "C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error using SHAP with RNN: in user code:\n",
      "\n",
      "    File \"C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 244, in grad_graph  *\n",
      "        out = self.model(shap_rAnD)\n",
      "    File \"C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 371, in custom_grad\n",
      "        out = op_handlers[type_name](self, op, *grads) # we cut off the shap_ prefix before the lookup\n",
      "    File \"C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 663, in handler\n",
      "        return linearity_with_excluded_handler(input_inds, explainer, op, *grads)\n",
      "    File \"C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 670, in linearity_with_excluded_handler\n",
      "        assert not explainer._variable_inputs(op)[i], str(i) + \"th input to \" + op.name + \" cannot vary!\"\n",
      "    File \"C:\\Users\\Muhammad Raihan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 222, in _variable_inputs\n",
      "        out[i] = t.name in self.between_tensors\n",
      "\n",
      "    AttributeError: Exception encountered when calling layer 'lstm' (type LSTM).\n",
      "    \n",
      "    'TFDeep' object has no attribute 'between_tensors'\n",
      "    \n",
      "    Call arguments received by layer 'lstm' (type LSTM):\n",
      "      • inputs=tf.Tensor(shape=(14032, 4, 1), dtype=float32)\n",
      "      • mask=None\n",
      "      • training=False\n",
      "      • initial_state=None\n",
      "\n",
      "\n",
      "Hasil Evaluasi ML/DL tanpa XAI:\n",
      "[['DecisionTree', 0.968476357267951, 0.9816568047337279, 0.9750220393770204, 0.9717326238776189, 0.06601619720458984], ['RandomForest', 0.9741632413388138, 0.9816568047337279, 0.97789566755084, 0.9750581975390755, 1.8729915618896484], ['Logistic Regression', 0.8135330578512396, 0.9319526627218935, 0.8687258687258687, 0.8417026937146658, 0.2021188735961914], ['Naive Bayes', 0.9592901878914405, 0.5437869822485207, 0.6941087613293051, 0.7306285334220153, 0.004999637603759766], ['MLP', 0.5624791736087971, 0.9988165680473373, 0.719675975271797, 0.562687063518457, 2.7650022506713867], ['Stochastic Gradient Descent', 0.8241758241758241, 0.7544378698224852, 0.7877664504170527, 0.7715330894579315, 0.07656121253967285], ['ADA Boost', 0.9560117302052786, 0.9644970414201184, 0.9602356406480118, 0.9551047555703359, 2.3133418560028076], ['Gradient Boosting', 0.9722222222222222, 0.9733727810650887, 0.9727971614429333, 0.9694047223145993, 1.889233112335205], ['XGBoost', 0.9782224838140082, 0.9834319526627219, 0.9808203009737385, 0.9783837712005321, 23.37704110145569], ['LightGBM', 0.9765944997074313, 0.9875739644970414, 0.9820535451603413, 0.9797140006651147, 73.66367077827454], ['CatBoost', 0.9704142011834319, 0.9704142011834319, 0.9704142011834319, 0.966744263385434, 2.851841688156128], ['DNN', 0.8004938271604938, 0.9591715976331361, 0.872678331090175, 0.8427003658131028, 10.275338649749756], ['CNN', 0.9162045594577942, 0.8798816568047337, 0.8976758225173559, 0.8872630528766212, 10.955185890197754], ['RNN', 0.9056271981242673, 0.9142011834319527, 0.9098939929328622, 0.898237445959428, 25.744317293167114]]\n",
      "\n",
      "Hasil Evaluasi ML/DL dengan XAI:\n",
      "[['DecisionTree', 0.9677608440797186, 0.9769230769230769, 0.9723203769140164, 0.9687396075823079, 0.03952765464782715, {'SHAP': array([[-0.0110954 ,  0.0110954 ],\n",
      "       [ 0.00199353, -0.00199353],\n",
      "       [-0.01904487,  0.01904487],\n",
      "       [-0.00240074,  0.00240074]])}], ['RandomForest', 0.9730205278592375, 0.9816568047337279, 0.977319587628866, 0.9743930828067842, 1.448897123336792, {'SHAP': array([[-0.00903142,  0.00903142],\n",
      "       [-0.00256726,  0.00256726],\n",
      "       [-0.01479315,  0.01479315],\n",
      "       [-0.00119234,  0.00119234]])}], ['Logistic Regression', 0.8135330578512396, 0.9319526627218935, 0.8687258687258687, 0.8417026937146658, 0.1686112880706787, {'SHAP': array([[-0.00099555,  0.00099555],\n",
      "       [ 0.00028692, -0.00028692],\n",
      "       [-0.03951409,  0.03951409],\n",
      "       [-0.00056555,  0.00056555]])}], ['Naive Bayes', 0.9592901878914405, 0.5437869822485207, 0.6941087613293051, 0.7306285334220153, 0.045592308044433594, {'SHAP': array([[-0.00717924,  0.00717924],\n",
      "       [ 0.02157755, -0.02157755],\n",
      "       [-0.02408096,  0.02408096],\n",
      "       [-0.00050675,  0.00050675]])}], ['MLP', 0.5624791736087971, 0.9988165680473373, 0.719675975271797, 0.562687063518457, 2.11020827293396, {'SHAP': array([[ 0.00633835, -0.00633835],\n",
      "       [ 0.00692791, -0.00692791],\n",
      "       [ 0.00047095, -0.00047095],\n",
      "       [ 0.00013877, -0.00013877]])}], ['Stochastic Gradient Descent', 0.8241758241758241, 0.7544378698224852, 0.7877664504170527, 0.7715330894579315, 0.09055185317993164, {'SHAP': array([[ 1.07609269e-02, -1.07609269e-02],\n",
      "       [ 1.03084548e-03, -1.03084548e-03],\n",
      "       [-4.61004271e-02,  4.61004271e-02],\n",
      "       [-3.12370620e-05,  3.12370620e-05]])}], ['ADA Boost', 0.9560117302052786, 0.9644970414201184, 0.9602356406480118, 0.9551047555703359, 1.8928894996643066, {'SHAP': array([[-0.00026356,  0.00026356],\n",
      "       [ 0.0011529 , -0.0011529 ],\n",
      "       [-0.00068292,  0.00068292],\n",
      "       [ 0.0003373 , -0.0003373 ]])}], ['Gradient Boosting', 0.9722222222222222, 0.9733727810650887, 0.9727971614429333, 0.9694047223145993, 1.845066785812378, {'SHAP': array([[-0.00802804,  0.00802804],\n",
      "       [ 0.00763648, -0.00763648],\n",
      "       [-0.01266729,  0.01266729],\n",
      "       [ 0.00041164, -0.00041164]])}], ['XGBoost', 0.9782224838140082, 0.9834319526627219, 0.9808203009737385, 0.9783837712005321, 24.634255170822144, {'SHAP': array([[-0.01439793,  0.01439793],\n",
      "       [-0.005563  ,  0.005563  ],\n",
      "       [-0.01453844,  0.01453844],\n",
      "       [ 0.00093599, -0.00093599]])}], ['LightGBM', 0.9765944997074313, 0.9875739644970414, 0.9820535451603413, 0.9797140006651147, 27.88341975212097, {'SHAP': array([[-0.01343446,  0.01343446],\n",
      "       [-0.00198579,  0.00198579],\n",
      "       [-0.01256635,  0.01256635],\n",
      "       [-0.00394397,  0.00394397]])}], ['CatBoost', 0.9704142011834319, 0.9704142011834319, 0.9704142011834319, 0.966744263385434, 1.578129768371582, {'SHAP': array([[-0.00490788,  0.00490788],\n",
      "       [ 0.01194486, -0.01194486],\n",
      "       [-0.01886779,  0.01886779],\n",
      "       [ 0.00111475, -0.00111475]])}], ['DNN', 0.8911525029103609, 0.9059171597633137, 0.8984741784037559, 0.8849351513136016, 8.82326626777649, {'SHAP': array([0.11494099])}], ['CNN', 0.899796885578876, 0.7863905325443787, 0.8392800757814967, 0.830728300631859, 2.511430501937866, {'SHAP': None}], ['RNN', 0.9426987060998152, 0.9053254437869822, 0.9236341684274072, 0.915862986365148, 6.5942864418029785, {'SHAP': None}]]\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi variabel untuk menyimpan hasil evaluasi\n",
    "hasil_ml_dl = []\n",
    "hasil_ml_dl_xai = []\n",
    "\n",
    "# Encode labels ke bentuk numerik jika diperlukan\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Fungsi untuk mengevaluasi model ML/DL\n",
    "def EvaluateModel(model_name, model, X_train, y_train, X_test, y_test, use_xai=False, is_dl_model=False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Melatih model\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0) if is_dl_model else model.fit(X_train, y_train)\n",
    "    \n",
    "    if is_dl_model:\n",
    "        y_pred_proba = model.predict(X_test)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).ravel()\n",
    "    elif hasattr(model, 'predict_proba'):\n",
    "        # Model dengan metode predict_proba\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        if y_pred_proba.shape[1] > 1:  # Model klasifikasi multi-kelas\n",
    "            y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        else:  # Model klasifikasi biner\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int).ravel()\n",
    "    else:\n",
    "        # Model tanpa metode predict_proba\n",
    "        y_pred_proba = model.predict(X_test)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).ravel()\n",
    "\n",
    "    # Menghitung confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    # Menghitung metrik\n",
    "    Precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    Recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    F1Score = 2 * ((Precision * Recall) / (Precision + Recall)) if (Precision + Recall) != 0 else 0\n",
    "    Accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) != 0 else 0\n",
    "\n",
    "    # Menghitung waktu running\n",
    "    run_time = time.time() - start_time\n",
    "\n",
    "    # Jika XAI diperlukan, tambahkan analisis dengan SHAP\n",
    "    if use_xai:\n",
    "        # Periksa apakah X_train adalah DataFrame\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            feature_names = X_train.columns\n",
    "        else:\n",
    "            feature_names = [f\"Feature_{i}\" for i in range(X_train.shape[1])]\n",
    "        \n",
    "        # Perbaiki format X_train untuk SHAP\n",
    "        if is_dl_model:\n",
    "            X_train_for_xai = X_train.reshape((X_train.shape[0], X_train.shape[1]))\n",
    "            X_test_for_xai = X_test.reshape((X_test.shape[0], X_test.shape[1]))\n",
    "        else:\n",
    "            X_train_for_xai = X_train\n",
    "            X_test_for_xai = X_test\n",
    "\n",
    "        # Gunakan SHAP\n",
    "        try:\n",
    "            if is_dl_model:\n",
    "                explainer = shap.DeepExplainer(model, X_train_for_xai)\n",
    "                shap_values = explainer.shap_values(X_test_for_xai)\n",
    "                shap_summary = np.mean(shap_values[0], axis=0)\n",
    "            else:\n",
    "                explainer = shap.Explainer(model.predict_proba, X_train_for_xai)\n",
    "                shap_values = explainer(X_test_for_xai)\n",
    "                shap_summary = shap_values.values.mean(axis=0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error using SHAP with {model_name}: {e}\")\n",
    "            shap_summary = None\n",
    "\n",
    "        # Simpan hasil evaluasi dengan XAI\n",
    "        hasil_ml_dl_xai.append([model_name, Precision, Recall, F1Score, Accuracy, run_time, {'SHAP': shap_summary}])\n",
    "    else:\n",
    "        # Simpan hasil evaluasi tanpa XAI\n",
    "        hasil_ml_dl.append([model_name, Precision, Recall, F1Score, Accuracy, run_time])\n",
    "\n",
    "# Model ML dan DL yang akan dievaluasi\n",
    "model_ml_dl = {\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=10),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=50),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=0, max_iter=10000),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"MLP\": MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=1000),\n",
    "    \"Stochastic Gradient Descent\": SGDClassifier(loss='log_loss', random_state=42),\n",
    "    \"ADA Boost\": AdaBoostClassifier(n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=100),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='Logloss')\n",
    "}\n",
    "\n",
    "model_dl = {\n",
    "    \"DNN\": Sequential([\n",
    "        Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ]),\n",
    "    \"CNN\": Sequential([\n",
    "        Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ]),\n",
    "    \"RNN\": Sequential([\n",
    "        LSTM(100, input_shape=(X_train.shape[1], 1)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Pastikan X_train dan X_test memiliki bentuk yang sesuai untuk DL\n",
    "X_train_dl = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_dl = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Mengevaluasi model ML tanpa XAI\n",
    "for model_name, model in model_ml_dl.items():\n",
    "    EvaluateModel(model_name, model, X_train, y_train_encoded, X_test, y_test_encoded, use_xai=False)\n",
    "\n",
    "# Mengevaluasi model DL tanpa XAI\n",
    "for model_name, model in model_dl.items():\n",
    "    model.compile(optimizer='adam', loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "    EvaluateModel(model_name, model, X_train_dl, y_train_encoded, X_test_dl, y_test_encoded, use_xai=False, is_dl_model=True)\n",
    "\n",
    "# Mengevaluasi model ML dengan XAI\n",
    "for model_name, model in model_ml_dl.items():\n",
    "    EvaluateModel(model_name, model, X_train, y_train_encoded, X_test, y_test_encoded, use_xai=True)\n",
    "\n",
    "# Mengevaluasi model DL dengan XAI\n",
    "for model_name, model in model_dl.items():\n",
    "    model.compile(optimizer='adam', loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "    EvaluateModel(model_name, model, X_train_dl, y_train_encoded, X_test_dl, y_test_encoded, use_xai=True, is_dl_model=True)\n",
    "\n",
    "# Print hasil evaluasi tanpa XAI\n",
    "print(\"\\nHasil Evaluasi ML/DL tanpa XAI:\")\n",
    "print(hasil_ml_dl)\n",
    "\n",
    "# Print hasil evaluasi dengan XAI\n",
    "print(\"\\nHasil Evaluasi ML/DL dengan XAI:\")\n",
    "print(hasil_ml_dl_xai)\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "df_ml_dl = pd.DataFrame(hasil_ml_dl, columns=[\"Model\", \"Precision\", \"Recall\", \"F1Score\", \"Accuracy\", \"RunTime\"])\n",
    "df_ml_dl_xai = pd.DataFrame(hasil_ml_dl_xai, columns=[\"Model\", \"Precision\", \"Recall\", \"F1Score\", \"Accuracy\", \"RunTime\", \"XAI\"])\n",
    "\n",
    "df_ml_dl.to_csv(\"hasil_evaluasi_ml_dl_ETFC.csv\", index=False)\n",
    "df_ml_dl_xai.to_csv(\"hasil_evaluasi_ml_dl_xai_ETFC.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
